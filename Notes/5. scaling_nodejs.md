# Scaling Node.js (Cluster, Caching, Load Balancing)

## 1. Why Scaling is Needed?
- Node.js runs on a single thread (event loop).
- By default, it only uses one CPU core — not optimal for multi-core servers.
- High traffic can overload a single instance — need scalability strategies.

---

## 2. Cluster Module (Multi-core Utilization)
- Node.js has a built-in `cluster` module to spawn multiple worker processes.
- Each worker runs on a separate core, sharing the same server port.
- Master process handles load distribution; workers handle requests.

**Example:**
```js
const cluster = require("cluster");
const os = require("os");
const http = require("http");

if (cluster.isMaster) {
  const numCPUs = os.cpus().length;
  console.log(`Master process running on ${process.pid}`);

  // Fork workers
  for (let i = 0; i < numCPUs; i++) {
    cluster.fork();
  }

  // Restart workers if they crash
  cluster.on("exit", (worker) => {
    console.log(`Worker ${worker.process.pid} died. Restarting...`);
    cluster.fork();
  });

} else {
  // Worker processes
  http.createServer((req, res) => {
    res.writeHead(200);
    res.end(`Hello from worker ${process.pid}\n`);
  }).listen(3000);
}
```

**Pros:** Utilizes all CPU cores  
**Cons:** Workers don’t share memory (need Redis/DB for shared state)

---

## 3. Caching (Speeding up Responses)
- Caching reduces load on DB and improves response times.

**Types:**
- **In-memory cache (per instance):**  
  Example: `node-cache`, `memory-cache`  
  Fast, but lost on restart and not shared across instances.
- **Distributed cache:**  
  Example: Redis, Memcached  
  Centralized store accessible by all Node.js instances.  
  Great for scaling across clusters/multiple servers.

**Example with Redis (using ioredis):**
```js
const Redis = require("ioredis");
const redis = new Redis();

async function getUser(id) {
  const cacheKey = `user:${id}`;
  let user = await redis.get(cacheKey);

  if (user) return JSON.parse(user);

  // Simulate DB call
  user = { id, name: "Alice" };
  
  await redis.set(cacheKey, JSON.stringify(user), "EX", 3600); // expire in 1h
  return user;
}
```

**Pros:** Reduces DB load, faster response times  
**Cons:** Cache invalidation complexity, needs extra infra

---

## 4. Load Balancing (Distributing Traffic)
- When running multiple Node.js instances (clusters or across servers), you need a load balancer.

**Options:**
- **Reverse Proxy (NGINX / Apache):**  
  Routes requests to multiple Node.js instances.  
  Can also handle SSL termination, caching, compression.

  ```nginx
  upstream app_servers {
      server 127.0.0.1:3000;
      server 127.0.0.1:3001;
  }

  server {
      listen 80;
      location / {
          proxy_pass http://app_servers;
          proxy_set_header Host $host;
          proxy_set_header X-Real-IP $remote_addr;
      }
  }
  ```

- **Cloud Load Balancers:**  
  AWS ELB, GCP Load Balancer, Azure ALB.  
  Handle scaling across multiple VMs/containers.

- **PM2 Process Manager:**  
  Provides clustering + load balancing out of the box:  
  `pm2 start app.js -i max`  
  (spawns as many processes as CPU cores).

---

## 5. Scaling Strategy Summary
- **Vertical Scaling:** Bigger machine — limited by hardware.
- **Horizontal Scaling:** More instances + load balancing — better for large traffic.
- Combine Cluster + Caching + Load Balancing for production-ready scaling.

---

## 6. Quick Recap for Interview
- **Cluster:** Uses multiple CPU cores (built-in Node.js module).
- **Caching:** Use Redis/Memcached to reduce DB hits and speed up responses.
- **Load Balancing:** Use NGINX/PM2/Cloud LB to distribute requests across instances.
- **Best practice:** Stateless Node.js instances + Redis (shared cache) + NGINX (load balancer).