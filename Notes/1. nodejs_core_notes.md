
# Node.js Core Concepts (Interview Notes)

## 1. Event Loop

The **event loop** is the mechanism that allows Node.js to perform **non-blocking I/O** operations, despite being single-threaded.

### Phases of Event Loop:
1. **Timers** → executes callbacks from `setTimeout` & `setInterval`  
2. **Pending callbacks** → executes I/O callbacks deferred from previous cycle  
3. **Idle/prepare** → internal use  
4. **Poll** → retrieves new I/O events, executes I/O related callbacks  
5. **Check** → executes `setImmediate` callbacks  
6. **Close callbacks** → executes e.g. `socket.on('close')`  

Alongside, Node.js has **microtask queue** (Promises, `process.nextTick`) which runs **between phases**, always before moving back to the event loop.

### Example:
```js
setTimeout(() => console.log("setTimeout"), 0);
setImmediate(() => console.log("setImmediate"));
process.nextTick(() => console.log("nextTick"));
Promise.resolve().then(() => console.log("Promise"));

console.log("Start");
```

**Order of execution:**
```
Start
nextTick
Promise
setTimeout
setImmediate
```

---

## 2. Buffers

1. A Buffer is a temporary storage (raw binary data) in Node.js.

2. It is used when dealing with binary data like files, TCP streams, or images.

3. Buffers are fixed in size once allocated.

4. You can read/write bytes directly (buffer[0] = 0x61 → 'a').

```js
const buffer = Buffer.from('Hello');
console.log(buffer);        // <Buffer 48 65 6c 6c 6f>
console.log(buffer.toString()); // "Hello"
```

## 2. Streams

**Streams** are **abstract interfaces** for working with data in chunks (instead of loading everything in memory).

### Types of Streams:
1. **Readable** → data can be read (`fs.createReadStream`)  
2. **Writable** → data can be written (`fs.createWriteStream`)  
3. **Duplex** → both read & write (e.g., TCP socket)  
4. **Transform** → duplex + transformation (e.g., `zlib.createGzip`)  

### Example:
```js
const fs = require("fs");

const readStream = fs.createReadStream("input.txt", { encoding: "utf8" });
const writeStream = fs.createWriteStream("output.txt");

readStream.pipe(writeStream); // Efficient: no buffering entire file
```

✅ Streams are memory-efficient & great for large files or real-time processing.

---

## 3. Cluster

By default, Node.js apps run in a **single thread**.  
The **Cluster module** allows you to spawn **multiple worker processes** that share the same server port.

### Example:
```js
const cluster = require("cluster");
const http = require("http");
const os = require("os");

if (cluster.isMaster) {
  const numCPUs = os.cpus().length;
  console.log(`Master ${process.pid} is running`);

  for (let i = 0; i < numCPUs; i++) {
    cluster.fork();
  }

  cluster.on("exit", (worker) => {
    console.log(`Worker ${worker.process.pid} died`);
    cluster.fork(); // Restart worker
  });

} else {
  http.createServer((req, res) => {
    res.writeHead(200);
    res.end(`Hello from Worker ${process.pid}\n`);
  }).listen(3000);

  console.log(`Worker ${process.pid} started`);
}
```

⚡ This way, Node.js can **scale across CPU cores**.

### Cluster vs Worker Threads:
- **Cluster** = multiple processes, separate memory  
- **Worker Threads** = multiple threads in same process, shared memory (better for CPU-bound tasks)  

---

## Quick Recap

- **Event Loop**: manages async execution in phases (timers → I/O → check → close), with **microtasks (Promises, nextTick)** running before macrotasks.  
- **Streams**: process data in chunks (Readable, Writable, Duplex, Transform). Efficient for large I/O.  
- **Cluster**: enables multi-core utilization by forking worker processes that share the same server port.
